encoder: transformer
encoder_conf:
    input_layer: conv2d
    num_blocks: 12
    linear_units: 2048
    dropout_rate: 0.1
    output_size: 256  # dimension of attention
    attention_heads: 4
    attention_dropout_rate: 0.0

decoder: transformer
decoder_conf:
    attention_heads: 4
    linear_units: 2048
    num_blocks: 6
    dropout_rate: 0.1
    positional_dropout_rate: 0.1
    self_attention_dropout_rate: 0.0
    src_attention_dropout_rate: 0.0

# extra_asr_decoder: transformer
# extra_asr_decoder_conf:
#     input_layer: embed
#     num_blocks: 2
#     linear_units: 2048
#     dropout_rate: 0.1

# extra_mt_decoder: transformer
# extra_mt_decoder_conf:
#     input_layer: embed
#     num_blocks: 2
#     linear_units: 2048
#     dropout_rate: 0.1

# loss related
model_conf:
    asr_weight: 0.1
    mt_weight: 0.0
    mtlalpha: 1.0  # ctc vs att_asr
    lsm_weight: 0.1
    length_normalized_loss: false

# optimization related
optim: adam
optim_conf:
    lr: 0.0001
    betas:
      - 0.9
      - 0.98
    eps: 0.000000001
    weight_decay: 0.0001

accum_grad: 2
grad_clip: 5
init: xavier_uniform
batch_type: folded
batch_size: 64
max_epoch: 200
patience: none

scheduler: warmuplr
scheduler_conf:
    warmup_steps: 25000

best_model_criterion:
-   - valid
    - acc
    - max
keep_nbest_models: 10

# specaug: specaug
# specaug_conf:
#     apply_time_warp: true
#     time_warp_window: 5
#     time_warp_mode: bicubic
#     apply_freq_mask: true
#     freq_mask_width_range:
#     - 0
#     - 30
#     num_freq_mask: 2
#     apply_time_mask: true
#     time_mask_width_range:
#     - 0
#     - 40
#     num_time_mask: 2
